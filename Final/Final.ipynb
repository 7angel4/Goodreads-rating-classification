{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aa0dcc",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2023 Semester 1\n",
    "\n",
    "## Assignment 2: Book Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b7d63",
   "metadata": {},
   "source": [
    "**Student IDs:**     1264462 & 1269549"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee271d",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "* To run the complete code in Jupyter Notebook, simply click Kernel > Restart & Run All.\n",
    "    * Note that it might take a while to generate the output - the slowest model takes about < 7 minutes.\n",
    "    * Alternatively, select the relevant cell, and click 'Run'.\n",
    "* Code lines commented out with `##` are for printing out information that may be of interest. You may uncomment it to see the output.\n",
    "* The dataset files (both the original and preprocessed) are assumed to exist in a folder named '**Datasets**', which resides in the same directory as this notebook.\n",
    "* There are two helper notebooks in the same directory, namely \"TextPreprocessing.ipynb\" and \"HelperFunctions.ipynb\". Here we make use of the IPython built-in magic command (`%run`) to run these files. You do not need to open and run them separately.\n",
    "* This code makes use of a library \"mlxtend\". To install it, run in the terminal: `pip install mlxtend`.\n",
    "    * More details about this library can be found here: https://rasbt.github.io/mlxtend/.\n",
    "* Each model may use a different version of the preprocessed dataset. Running this code generates the \"train_df_ohe.csv\" and \"train_df_oe.csv\" file. \n",
    "    * We have generated the other versions beforehand (\"train_df_ohe_300\", \"train_df_oe_300\"), which only requires a small change in a function parameter.\n",
    "    * All required, preprocessed datasets lie in the 'Datasets' folder.\n",
    "* This code also generates a file of the format \"[model]_pred.csv\", which contains the final predictions made by the selected model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14efbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import chi2, SelectKBest, SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e2c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d030454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run TextPreprocessing.ipynb\n",
    "%run HelperFunctions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f55cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"./Datasets/\"\n",
    "CLASS_LABEL = \"rating_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81812266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(DATASET_DIR + \"book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "test_df = pd.read_csv(DATASET_DIR + \"book_rating_test.csv\", index_col = False, delimiter = ',', header=0)\n",
    "entire_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e25a82",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4fe86",
   "metadata": {},
   "source": [
    "Note: Here we show the individual preprocessing steps for the training set for clarity. At the end we pull everything together to preprocess the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b1fa6",
   "metadata": {},
   "source": [
    "## Preprocess String features: Authors and Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58dcc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = ['Authors', 'Publisher']\n",
    "MISSING_CAT_VAL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7ab1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('Language', axis=1)  # this feature has too many missing values\n",
    "\n",
    "for df in [train_df, entire_df]:\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].fillna(MISSING_CAT_VAL)  # impute missing values\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(x, stop_words_removal=False, lemmatize=False, min_word_len=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d720b85",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d64489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;infrequent_if_exist&#x27;, min_frequency=3,\n",
       "              sparse=False, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;infrequent_if_exist&#x27;, min_frequency=3,\n",
       "              sparse=False, sparse_output=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=3,\n",
       "              sparse=False, sparse_output=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "OHE = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=3, sparse=False)\n",
    "OHE.fit(entire_df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82b3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_transform(df, has_labels=False):\n",
    "    \"\"\"\n",
    "    Transforms the dataset by one-hot-encoding, on the categorical features only.\n",
    "    `has_label` indicates whether the DataFrame contains the class labels.\n",
    "        If True, these are moved to the last column.\n",
    "    Returns the transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformed_mat = OHE.transform(df[CATEGORICAL_FEATURES])\n",
    "    transformed_cat_df = pd.DataFrame(transformed_mat).set_axis(OHE.get_feature_names_out(), axis=1, inplace=False)\n",
    "    transformed_df = pd.concat([df.reset_index(drop=True), transformed_cat_df], axis=1)\n",
    "    transformed_df = transformed_df.drop(CATEGORICAL_FEATURES, axis=1)  # drop the original attributes\n",
    "    \n",
    "    if has_labels:\n",
    "        # move rating_label to the last column\n",
    "        labels = transformed_df[CLASS_LABEL]\n",
    "        transformed_df = pd.concat([transformed_df.drop([CLASS_LABEL], axis=1), labels], axis=1)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd0c918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497 features are significant.\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "# Select one-hot-encoded features using chi2\n",
    "x2 = SelectKBest(chi2, k='all')\n",
    "x2.fit(ohe_transform(train_df)[OHE.get_feature_names_out()], train_df[CLASS_LABEL])\n",
    "pvals = pd.DataFrame(x2.pvalues_, index=x2.feature_names_in_, columns=['p-value'])\n",
    "## print(pvals)\n",
    "\n",
    "INSIG_OHE_FEATURES = pvals[pvals['p-value'] >= ALPHA].index.tolist()  # insignificant encoded features\n",
    "sig_ohe_features = pvals[pvals['p-value'] < ALPHA].index.tolist()\n",
    "print(len(sig_ohe_features), \"features are significant.\")\n",
    "## sig_ohe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37dcd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ohe_transform(train_df, has_labels=True)\n",
    "train_df = train_df.drop(INSIG_OHE_FEATURES, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e29f9",
   "metadata": {},
   "source": [
    "## Discretize 'Numerical' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58895b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize(df, feature, discretizer=None, strategy='kmeans', n_bins=5):\n",
    "    \"\"\"\n",
    "    Discretizes the feature in the given DataFrame.\n",
    "    - discretizer: the discretizer; \n",
    "      If None, creates a KBinsDiscretizer for ordinal data, with the specified strategy and number of bins.\n",
    "    - strategy: the discretization strategy (one of ['kmeans', 'quantile', 'uniform']).\n",
    "    - n_bins: the number of bins.\n",
    "    Returns the transformed dataset and discretizer used.\n",
    "    \"\"\"\n",
    "    \n",
    "    if discretizer is None:\n",
    "        discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "        discretizer.fit(df[[feature]])\n",
    "        \n",
    "    transformed_df = df.copy()\n",
    "    transformed_df[feature] = discretizer.transform(df[[feature]])\n",
    "    \n",
    "    return transformed_df, discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7136bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PublishYear\n",
    "train_df, DISCRETIZER_PY = discretize(train_df, 'PublishYear', strategy='uniform', n_bins=15)\n",
    "\n",
    "# PublishMonth\n",
    "train_df, DISCRETIZER_PM = discretize(train_df, 'PublishMonth', strategy='quantile', n_bins=6)\n",
    "\n",
    "# PublishDay\n",
    "train_df, DISCRETIZER_PD = discretize(train_df, 'PublishDay', strategy='quantile', n_bins=11)\n",
    "\n",
    "# pagesNumber\n",
    "train_df, DISCRETIZER_PN = discretize(train_df, 'pagesNumber', strategy='kmeans', n_bins=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95929a9",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f1faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FEATURES = ['Name', 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7ce7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max_features=300 to generate the dataset version for the Logistic Regression classifier\n",
    "train_df_name, VECTORIZER_NAME = preprocess_text_feature(train_df, 'Name', ngram=2, delimiter='_')  # distinguish from Description features\n",
    "train_df_desc, VECTORIZER_DESC = preprocess_text_feature(train_df, 'Description', ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aea491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind them together\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_df_name, train_df_desc], axis=1)\n",
    "train_df = train_df.drop(TEXT_FEATURES, axis=1)  # drop the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b7247bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move rating_label to the last column\n",
    "labels = train_df[CLASS_LABEL].astype('category')\n",
    "train_df = pd.concat([train_df.drop([CLASS_LABEL], axis=1), labels], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c89ac",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f75eac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_df_X = train_df.iloc[:,:-1]\n",
    "train_df_y = train_df.iloc[:,-1]\n",
    "SCALER = MinMaxScaler()\n",
    "scaled = SCALER.fit_transform(train_df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc9fd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>PublishMonth</th>\n",
       "      <th>PublishDay</th>\n",
       "      <th>pagesNumber</th>\n",
       "      <th>Authors_</th>\n",
       "      <th>Authors_aaron mccollough</th>\n",
       "      <th>Authors_abu hamid al ghazali</th>\n",
       "      <th>Authors_agatha christie</th>\n",
       "      <th>Authors_alba de c spedes</th>\n",
       "      <th>Authors_alexander kent</th>\n",
       "      <th>...</th>\n",
       "      <th>book offer</th>\n",
       "      <th>young woman</th>\n",
       "      <th>science fiction</th>\n",
       "      <th>well known</th>\n",
       "      <th>br em</th>\n",
       "      <th>real life</th>\n",
       "      <th>tell story</th>\n",
       "      <th>african american</th>\n",
       "      <th>san francisco</th>\n",
       "      <th>rating_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23058</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23059</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23060</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23061</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23062</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23063 rows × 609 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PublishYear  PublishMonth  PublishDay  pagesNumber  Authors_  \\\n",
       "0         0.928571           0.6    0.000000     0.000000       0.0   \n",
       "1         0.857143           1.0    0.000000     0.333333       0.0   \n",
       "2         0.928571           0.2    1.000000     0.000000       0.0   \n",
       "3         0.928571           0.8    0.000000     0.166667       0.0   \n",
       "4         0.928571           0.6    0.166667     0.166667       0.0   \n",
       "...            ...           ...         ...          ...       ...   \n",
       "23058     0.857143           0.6    0.000000     0.000000       0.0   \n",
       "23059     0.928571           0.6    0.000000     0.000000       0.0   \n",
       "23060     0.857143           0.2    0.500000     0.000000       0.0   \n",
       "23061     0.928571           0.4    0.666667     0.000000       0.0   \n",
       "23062     0.928571           0.6    0.333333     0.000000       0.0   \n",
       "\n",
       "       Authors_aaron mccollough  Authors_abu hamid al ghazali  \\\n",
       "0                           0.0                           0.0   \n",
       "1                           0.0                           0.0   \n",
       "2                           0.0                           0.0   \n",
       "3                           0.0                           0.0   \n",
       "4                           0.0                           0.0   \n",
       "...                         ...                           ...   \n",
       "23058                       0.0                           0.0   \n",
       "23059                       0.0                           0.0   \n",
       "23060                       0.0                           0.0   \n",
       "23061                       0.0                           0.0   \n",
       "23062                       0.0                           0.0   \n",
       "\n",
       "       Authors_agatha christie  Authors_alba de c spedes  \\\n",
       "0                          0.0                       0.0   \n",
       "1                          0.0                       0.0   \n",
       "2                          0.0                       0.0   \n",
       "3                          0.0                       0.0   \n",
       "4                          0.0                       0.0   \n",
       "...                        ...                       ...   \n",
       "23058                      0.0                       0.0   \n",
       "23059                      0.0                       0.0   \n",
       "23060                      0.0                       0.0   \n",
       "23061                      0.0                       0.0   \n",
       "23062                      0.0                       0.0   \n",
       "\n",
       "       Authors_alexander kent  ...  book offer  young woman  science fiction  \\\n",
       "0                         0.0  ...         0.0          0.0              0.0   \n",
       "1                         0.0  ...         0.0          0.0              0.0   \n",
       "2                         0.0  ...         0.0          0.0              0.0   \n",
       "3                         0.0  ...         0.0          0.0              0.0   \n",
       "4                         0.0  ...         0.0          0.0              0.0   \n",
       "...                       ...  ...         ...          ...              ...   \n",
       "23058                     0.0  ...         0.0          0.0              0.0   \n",
       "23059                     0.0  ...         0.0          0.0              0.0   \n",
       "23060                     0.0  ...         0.0          0.0              0.0   \n",
       "23061                     0.0  ...         0.0          0.0              0.0   \n",
       "23062                     0.0  ...         0.0          0.0              0.0   \n",
       "\n",
       "       well known  br em  real life  tell story  african american  \\\n",
       "0             0.0    0.0        0.0         0.0               0.0   \n",
       "1             0.0    0.0        0.0         0.0               0.0   \n",
       "2             0.0    0.0        0.0         0.0               0.0   \n",
       "3             0.0    0.0        0.0         0.0               0.0   \n",
       "4             0.0    0.0        0.0         0.0               0.0   \n",
       "...           ...    ...        ...         ...               ...   \n",
       "23058         0.0    0.0        0.0         0.0               0.0   \n",
       "23059         0.0    0.0        0.0         0.0               0.0   \n",
       "23060         0.0    0.0        0.0         0.0               0.0   \n",
       "23061         0.0    0.0        0.0         0.0               0.0   \n",
       "23062         0.0    1.0        0.0         0.0               0.0   \n",
       "\n",
       "       san francisco  rating_label  \n",
       "0                0.0           4.0  \n",
       "1                0.0           4.0  \n",
       "2                0.0           4.0  \n",
       "3                0.0           4.0  \n",
       "4                0.0           3.0  \n",
       "...              ...           ...  \n",
       "23058            0.0           4.0  \n",
       "23059            0.0           4.0  \n",
       "23060            0.0           4.0  \n",
       "23061            0.0           4.0  \n",
       "23062            0.0           4.0  \n",
       "\n",
       "[23063 rows x 609 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X = pd.DataFrame(scaled, columns=train_df_X.columns)\n",
    "train_df = pd.concat([scaled_X, train_df_y], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aed2f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed training set\n",
    "train_df.to_csv(DATASET_DIR + \"train_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a06799",
   "metadata": {},
   "source": [
    "## Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c99b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_df(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the test dataset and returns the preprocessed version as a DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # preprocess string features\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].fillna(MISSING_CAT_VAL)  # imputation\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(str(x), stop_words_removal=False, lemmatize=False, min_word_len=0))\n",
    "    df = ohe_transform(df).drop(INSIG_OHE_FEATURES, axis=1)  # one-hot-encoding\n",
    "    \n",
    "    # preprocess numerical features\n",
    "    df = discretize(df, 'PublishDay', discretizer=DISCRETIZER_PD)[0]\n",
    "    df = discretize(df, 'PublishMonth', discretizer=DISCRETIZER_PM)[0]\n",
    "    df = discretize(df, 'PublishYear', discretizer=DISCRETIZER_PY)[0]\n",
    "    df = discretize(df, 'pagesNumber', discretizer=DISCRETIZER_PN)[0]\n",
    "    \n",
    "    # preprocess text features\n",
    "    df_name = preprocess_text_feature(df, 'Name', vectorizer=VECTORIZER_NAME, delimiter='_')[0]\n",
    "    df_desc = preprocess_text_feature(df, 'Description', vectorizer=VECTORIZER_DESC)[0]\n",
    "    \n",
    "    # discard the obsolete original features and unwanted features\n",
    "    df = pd.concat([df.reset_index(drop=True), df_name, df_desc], axis=1)\n",
    "    df = df.drop(['Name', 'Description', 'Language'], axis=1)\n",
    "    \n",
    "    # normalize\n",
    "    df = pd.DataFrame(SCALER.transform(df), columns=df.columns)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e403a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the test set\n",
    "test_df = preprocess_test_df(test_df)\n",
    "test_df.to_csv(DATASET_DIR + \"test_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fdc3b1",
   "metadata": {},
   "source": [
    "# 2. Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd0157",
   "metadata": {},
   "source": [
    "Training sets for each model:\n",
    "- `logr`: train_df_ohe_300.csv, the one-hot-encoded dataset with 300 'Name' and 'Description' features, respectively.\n",
    "        This is generated by setting `max_features=300` in the `preprocess_text_feature()` function.\n",
    "- `rf`, `cvsnb`, baselines: train_df_ohe.csv, the one-hot-encoded dataset with 50 'Name' and 'Description' features, respectively.\n",
    "- `sclf`: train_df_oe.csv, the ordinal-encoded dataset with 50 'Name' and 'Description' features, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5a55ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_df_ohe = pd.read_csv(DATASET_DIR + \"train_df_ohe.csv\", keep_default_na=False)  # Alternative\n",
    "X_train = train_df.iloc[:,:-1]\n",
    "y_train = train_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe892a9",
   "metadata": {},
   "source": [
    "## 1) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbf9fe",
   "metadata": {},
   "source": [
    "## 2) StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaa832b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42773311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian NB\n",
    "gnb = GaussianNB()\n",
    "# Linear SVM\n",
    "linearSVM = svm.LinearSVC(random_state=30027, C=1)\n",
    "# Logistic Regression\n",
    "logr = LogisticRegression(solver='sag', max_iter=100)\n",
    "# Decision tree\n",
    "dt = DecisionTreeClassifier(max_depth = 400, random_state = 30027)\n",
    "# 3NN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Stack\n",
    "sclf = StackingCVClassifier(classifiers=[gnb, linearSVM, logr, dt, knn], \n",
    "                            meta_classifier=LogisticRegression(),\n",
    "                            cv=2,\n",
    "                            random_state=30027)\n",
    "\n",
    "sclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fc760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slow - be patient!\n",
    "y_pred_sclf, sclf_report = cross_val_report(sclf, X_train, y_train)\n",
    "sclf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f1c4a",
   "metadata": {},
   "source": [
    "## 3) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_logr = pd.read_csv(DATASET_DIR + \"train_df_ohe_300.csv\", keep_default_na=False)\n",
    "X_train_logr = train_df_logr.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208876cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression(solver='sag', max_iter=500)  # uses Stochastic Average Gradient descent solver\n",
    "logr.fit(X_train_logr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(logr, prefit=True).fit(X_train_logr, y_train)\n",
    "LR_FEATURES = [X_train_logr.columns[i] for i in selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ae062",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.fit(X_train_logr[LR_FEATURES], y_train)\n",
    "y_pred_logr, logr_report = cross_val_report(logr, X_train_logr[LR_FEATURES], y_train)\n",
    "logr_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d7b7e",
   "metadata": {},
   "source": [
    "## 4) Stacking Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAUS_FEATURES = ['pagesNumber', 'PublishYear']\n",
    "CAT_FEATURES = ['Authors']\n",
    "MN_FEATURES = chi2_select_features(X_train_oe[X_train_oe.columns[6:]], y_train_oe)  # only filtering the text features\n",
    "N_AUTHORS_VALS = len(ORD_ENCODER.categories_[0])  # number of unique Authors categories\n",
    "\n",
    "X_train_oe = X_train_oe[GAUS_FEATURES + CAT_FEATURES + MN_FEATURES]\n",
    "## X_train_oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = make_pipeline(ColumnSelector(GAUS_FEATURES),\n",
    "                      GaussianNB())\n",
    "cnb = make_pipeline(ColumnSelector(CAT_FEATURES),\n",
    "                      CategoricalNB(alpha=0.11, min_categories=N_AUTHORS_VALS))\n",
    "mnb = make_pipeline(ColumnSelector(MN_FEATURES),\n",
    "                      MultinomialNB(alpha=0.26))\n",
    "\n",
    "cvsnb = StackingCVClassifier(classifiers=[gnb, cnb, mnb], \n",
    "                            meta_classifier=CategoricalNB(),\n",
    "                            random_state=30027)\n",
    "\n",
    "cvsnb.fit(X_train_oe, y_train_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cvsnb, cvsnb_report = cross_val_report(cvsnb, X_train_oe, y_train_oe)\n",
    "cvsnb_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c844d6",
   "metadata": {},
   "source": [
    "## 5) Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1019aa6",
   "metadata": {},
   "source": [
    "### 0R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "zero_r = DummyClassifier(strategy='most_frequent')\n",
    "y_pred_zero_r, zero_r_report = cross_val_report(zero_r, X_train, y_train)\n",
    "zero_r_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13071b38",
   "metadata": {},
   "source": [
    "### 1R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "one_r = DecisionTreeClassifier(max_depth=1)\n",
    "y_pred_one_r, one_r_report = cross_val_report(one_r, X_train, y_train)\n",
    "one_r_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f522f",
   "metadata": {},
   "source": [
    "# 3. Final predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5330a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Clf\n",
    "X_test_sclf = pd.read_csv(DATASET_DIR + \"test_df_ohe.csv\", keep_default_na=False).drop(INSIG_OHE_FEATURES, axis=1)\n",
    "get_pred_df(sclf.predict(X_test_sclf)).to_csv(\"sclf_pred.csv\", index=False)\n",
    "pd.read_csv(\"sclf_pred.csv\")  # how the results look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "X_test_rf = pd.read_csv(DATASET_DIR + \"test_df_ohe.csv\", keep_default_na=False)\n",
    "get_pred_df(rf.predict(X_test_rf[RF_FEATURES])).to_csv(\"rf_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "X_test_logr = pd.read_csv(DATASET_DIR + \"test_df_ohe_300.csv\", keep_default_na=False)\n",
    "get_pred_df(logr.predict(X_test_logr[LR_FEATURES])).to_csv(\"logr_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9eff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking NB\n",
    "X_test_cvsnb = pd.read_csv(DATASET_DIR + \"test_df_oe.csv\", keep_default_na=False)\n",
    "get_pred_df(cvsnb.predict(X_test_cvsnb)).to_csv(\"cvsnb_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf9eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
