{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['3.0', '4.0', '5.0']\n",
    "PER_CLASS_METRICS = ['precision', 'recall', 'f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01739c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring function for cross validation\n",
    "def report_scoring(clf, X, y, include_avg=False): \n",
    "    \"\"\"\n",
    "    Returns a report of the classifier `clf`'s performance on the provided dataset.\n",
    "    The report is a dictionary, which includes the precision, recall, f1-score, and accuracy.\n",
    "    If `include_avg` is true, the report additionally includes the macro and weighted average \n",
    "    of the precision, recall, and f1-score.\n",
    "    \"\"\"\n",
    "    y_pred = clf.predict(X)\n",
    "    results = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    report_dict = {}  # set up our own output dictionary\n",
    "    for label in CLASSES: \n",
    "        for metric in ['precision', 'recall', 'f1-score']: \n",
    "            report_dict[label + '_' + metric] = results[label][metric]\n",
    "    report_dict['accuracy'] = results['accuracy']\n",
    "    \n",
    "    if include_avg:\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            report_dict['macro_avg_' + metric] = results['macro avg'][metric]\n",
    "            report_dict['weighted_avg_' + metric] = results['weighted avg'][metric]\n",
    "    \n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_report(clf, X, y, cv=10, print_full_results=True, print_confusion_matrix=True):\n",
    "    \"\"\"\n",
    "    Cross-validates the classifier on the given dataset.\n",
    "    - cv: number of folds in a StratifiedKFold cross-validation.\n",
    "    - print_full_results: whether to print the full cross-validation results.\n",
    "    - print_confusion_matrix: whether to print the confusion matrix.\n",
    "    Returns a 2-tuple consisting of:\n",
    "    1) a list for the cross-validated estimates for each input data point\n",
    "    2) a DataFrame for the aggregated cross-validation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    # cross validation on TRAINING set\n",
    "    result = cross_validate(clf, X, y, scoring=report_scoring, cv=cv)\n",
    "    end = time.time()\n",
    "    print(f\"{get_model_name(clf)} prediction took {end - start} seconds.\\n\")\n",
    "    \n",
    "    # Convert evaluation results to a dataframe\n",
    "    metrics = ['test_'+c+'_'+m for c in CLASSES for m in ['precision', 'recall', 'f1-score']]\n",
    "    metrics += ['test_accuracy']\n",
    "    results_df = pd.DataFrame.from_dict(result)[metrics].set_axis([metric[5:] for metric in metrics], axis=1)\n",
    "    \n",
    "    # Aggregate the results\n",
    "    agg_results = pd.concat([results_df.mean(axis=0), results_df.std(axis=0, ddof=1)], axis=1).set_axis(['mean', 'std'], axis=1)\n",
    "    \n",
    "    if print_full_results:\n",
    "        print(results_df)\n",
    "        \n",
    "    y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "    \n",
    "    if print_confusion_matrix:\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)\n",
    "        display.plot()\n",
    "        plt.show()\n",
    "    \n",
    "    return y_pred, agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeec879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_select_features(X, y, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Returns the selected features for the dataset using a chi-squared statistical test.\n",
    "    `alpha` is the significance level of the test.\n",
    "    \"\"\"\n",
    "    \n",
    "    x2 = SelectKBest(chi2, k='all')\n",
    "    x2.fit(X, y)\n",
    "    pvals = pd.DataFrame(x2.pvalues_, index=x2.feature_names_in_, columns=['p-value'])\n",
    "    x2_features = pvals[pvals['p-value'] < alpha].index.tolist()  # statistical test\n",
    "    return x2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(model):\n",
    "    \"\"\"\n",
    "    Returns the name of the model.\n",
    "    \"\"\"\n",
    "    return re.findall(r'(\\w+)\\(', str(model))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_df(y_pred):\n",
    "    \"\"\"\n",
    "    Converts the list of predictions to a DataFrame with 2 columns (id, rating_label).\n",
    "    Returns the resultant DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_df = pd.DataFrame(y_pred).reset_index().set_axis(['id', 'rating_label'], axis=1)\n",
    "    pred_df['id'] = pred_df['id']+1  # id starts from 1\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925526a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list(filename):\n",
    "    \"\"\"\n",
    "    Reads the file with the given `filename`.\n",
    "    Returns a list where each element is a line in the file.\n",
    "    \"\"\"\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    return [l.strip() for l in lines]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
