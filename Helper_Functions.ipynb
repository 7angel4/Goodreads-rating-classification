{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39665e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4d89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['3.0', '4.0', '5.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6925fa",
   "metadata": {},
   "source": [
    "# Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6742bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_vs_rating(df, feature):\n",
    "    plt.scatter(df[feature], df[CLASS_LABEL])\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a18ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(df, feature, bins=30):\n",
    "    plt.hist(df[feature], bins=bins)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754bada",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6c81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring for cross validation\n",
    "def report_scoring(clf, X, y, include_avg=False): \n",
    "    \"\"\"\n",
    "    Returns a report of the classifier `clf`'s performance on the provided dataset.\n",
    "    The report is a dictionary, which includes the precision, recall, f1-score, and accuracy.\n",
    "    If `include_avg` is true, the report additionally includes the macro and weighted average \n",
    "    of the precision, recall, and f1-score.\n",
    "    \"\"\"\n",
    "    y_pred = clf.predict(X)\n",
    "    results = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    report_dict = {}  # set up our own output dictionary\n",
    "    for label in CLASSES: \n",
    "        for metric in ['precision', 'recall', 'f1-score']: \n",
    "            report_dict[label + '_' + metric] = results[label][metric]\n",
    "    report_dict['accuracy'] = results['accuracy']\n",
    "    \n",
    "    if include_avg:\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            report_dict['macro_avg_' + metric] = results['macro avg'][metric]\n",
    "            report_dict['weighted_avg_' + metric] = results['weighted avg'][metric]\n",
    "    \n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0b90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation used for hyperparameter tuning and model selection\n",
    "def cross_val_report(clf, X, y, cv=10, print_full_results=True, print_confusion_matrix=True, predict=True):\n",
    "    \"\"\"\n",
    "    Cross-validates the classifier on the given dataset.\n",
    "    - cv: number of folds in a StratifiedKFold cross-validation.\n",
    "    - print_full_results: whether to print the full cross-validation results.\n",
    "    - print_confusion_matrix: whether to print the confusion matrix.\n",
    "    Returns a 2-tuple consisting of:\n",
    "    1) a list for the cross-validated estimates for each input data point\n",
    "    2) a DataFrame for the aggregated cross-validation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    # cross validation on TRAINING set\n",
    "    result = cross_validate(clf, X, y, scoring=report_scoring, cv=cv)\n",
    "    end = time.time()\n",
    "    print(f\"{get_model_name(clf)} prediction took {end - start} seconds.\\n\")\n",
    "    \n",
    "    # Convert evaluation results to a dataframe\n",
    "    metrics = ['test_'+c+'_'+m for c in CLASSES for m in ['precision', 'recall', 'f1-score']]\n",
    "    metrics += ['test_accuracy']\n",
    "    results_df = pd.DataFrame.from_dict(result)[metrics].set_axis([metric[5:] for metric in metrics], axis=1)\n",
    "    \n",
    "    # Aggregate the results\n",
    "    agg_results = pd.concat([results_df.mean(axis=0), results_df.std(axis=0, ddof=1)], axis=1).set_axis(['mean', 'std'], axis=1)\n",
    "    \n",
    "    if print_full_results:\n",
    "        print(results_df)\n",
    "    \n",
    "    y_pred = []\n",
    "    if predict: \n",
    "        y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "    \n",
    "    if print_confusion_matrix:\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)\n",
    "        display.plot()\n",
    "        plt.show()\n",
    "    \n",
    "    return y_pred, agg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d2a45",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6805c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(model):\n",
    "    \"\"\"\n",
    "    Returns the name of the model.\n",
    "    \"\"\"\n",
    "    return re.findall(r'(\\w+)\\(', str(model))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d865736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter(clf, param_grid, X, y, scoring=report_scoring, cv=20, refit = False): \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scoring, cv=cv, verbose=2, refit=refit, error_score=\"raise\")\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    result_df = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "    r = re.compile('^((mean|std|rank)_test|params).*')\n",
    "    score_metrics = list(filter(r.match, result_df.columns))\n",
    "    \n",
    "    return result_df[score_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_select_features(X, y, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Returns the selected features for the dataset using a chi-squared statistical test.\n",
    "    \"\"\"\n",
    "    \n",
    "    x2 = SelectKBest(chi2, k='all')\n",
    "    x2.fit(X, y)\n",
    "    pvals = pd.DataFrame(x2.pvalues_, index=x2.feature_names_in_, columns=['p-value'])\n",
    "    x2_features = pvals[pvals['p-value'] < alpha].index.tolist()  # statistical test at ALPHA significance level\n",
    "    return x2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355b953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
