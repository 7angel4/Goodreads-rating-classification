{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39665e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4d89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [3.0, 4.0, 5.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754bada",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring for cross validation\n",
    "def report_scoring(clf, X, y, include_avg=True): \n",
    "    y_pred = clf.predict(X)\n",
    "    results = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    report_dict = {}  # set up our own output dictionary\n",
    "    for label in ['3.0', '4.0', '5.0']: \n",
    "        for metric in ['precision', 'recall', 'f1-score']: \n",
    "            report_dict[label + '_' + metric] = results[label][metric]\n",
    "    report_dict['accuracy'] = results['accuracy']\n",
    "    \n",
    "    if include_avg:\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            report_dict['macro_avg_' + metric] = results['macro avg'][metric]\n",
    "            report_dict['weighted_avg_' + metric] = results['weighted avg'][metric]\n",
    "    \n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(model):\n",
    "    \"\"\"\n",
    "    Returns the name of the sklearn classifier.\n",
    "    \"\"\"\n",
    "    return re.findall(r'(\\w+)\\(', str(model))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0b90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(clf, X, y, print_full_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the classifier on the given dataset.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    # cross validation on TRAINING set\n",
    "    result = cross_validate(clf, X, y, scoring=report_scoring, cv=20, error_score = \"raise\")\n",
    "    end = time.time()\n",
    "    print(f\"{get_model_name(clf)} prediction took {end - start} seconds\")\n",
    "    \n",
    "    # Convert evaluation results to a dataframe (\"test_\" prefix is auto-generated)\n",
    "    results_df = pd.DataFrame.from_dict(result)[['test_3.0_f1-score','test_4.0_f1-score','test_5.0_f1-score','test_accuracy']]\n",
    "    if print_full_results:\n",
    "        print(results_df)\n",
    "    \n",
    "    return results_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb70578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X, y, print_confusion_matrix=True, print_classification_report=True):\n",
    "    y_pred = clf.predict(X)\n",
    "    if print_confusion_matrix:\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)\n",
    "        display.plot()\n",
    "        plt.show()\n",
    "    \n",
    "    if print_classification_report:\n",
    "        print(classification_report(y, y_pred, zero_division=0))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_test(clf, X_train, y_train, X_val, y_val, print_full_cv_results=True, print_confusion_matrix=True, print_classification_report=True):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Evaluate on training set:\\n\")\n",
    "    print(validate(clf, X_train, y_train, print_full_results=print_full_cv_results))\n",
    "    print(\"\\n\\nEvaluate on validation set:\\n\")\n",
    "    return evaluate(clf, X_val, y_val, print_confusion_matrix=print_confusion_matrix, print_classification_report=print_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d2a45",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring for hyperparameter tuning\n",
    "def hyperparameter_scoring(clf, X, y): \n",
    "    y_pred = clf.predict(X)\n",
    "    results = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    report_dict = {}  # set up our own output dictionary\n",
    "    for label in clf.classes_:\n",
    "        for metric in ['f1-score']: \n",
    "            report_dict[str(label) + '_' + metric] = results[str(label)][metric]\n",
    "    report_dict['accuracy'] = results['accuracy']\n",
    "    \n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d865736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter(clf, param_grid, X_train, y_train, scoring = None): \n",
    "    if scoring == None: \n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring=hyperparameter_scoring, cv=20, verbose=2, refit=False, error_score=\"raise\")\n",
    "        \n",
    "    else: \n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring=scoring, cv=20, verbose=2, refit=False, error_score=\"raise\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    result_df = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "    r = re.compile('^((mean|rank)_test|params).*')\n",
    "    score_metrics = list(filter(r.match, result_df.columns))\n",
    "    return result_df[score_metrics]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
