{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd73a2d",
   "metadata": {},
   "source": [
    "# Text-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855a346",
   "metadata": {},
   "source": [
    "## M1: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71434bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_count_vectorizer(feature, train_df, test_df, ngram=1):\n",
    "    train_corpus = train_df[feature]\n",
    "    test = test_df[feature]\n",
    "    \n",
    "    vocab = CountVectorizer(preprocessor=preprocess, min_df=2, max_df=len(train_df), ngram_range=(ngram,ngram)).fit(train_corpus)\n",
    "    x_train = vocab.transform(train_corpus)\n",
    "    x_test = vocab.transform(test)\n",
    "    \n",
    "    scipy.sparse.save_npz(f'./preprocessed_data/CountVectorizer/train_{feature}_{ngram}grams_countVectorizer.npz', x_train)\n",
    "    scipy.sparse.save_npz(f'./preprocessed_data/CountVectorizer/test_{feature}_{ngram}grams_countVectorizer.npz', x_test)\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d846fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_dict = vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199607b1",
   "metadata": {},
   "source": [
    "## M2: doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2601a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bd8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the output vector\n",
    "vec_size = 100\n",
    "\n",
    "# function to preprocess and tokenize text\n",
    "def tokenize_corpus(txt, tokens_only=False, ngram=1):\n",
    "    for i, line in enumerate(txt):\n",
    "        line_processed = preprocess(line)\n",
    "        if ngram == 1:\n",
    "            tokens = line_processed.split()\n",
    "        else:\n",
    "            tokens = list(ngrams(line_processed.split(), n=ngram))\n",
    "            tokens = [' '.join(token) for token in tokens]\n",
    "        # tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91f5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_doc2vec(feature, train_df, test_df, ngram=1):\n",
    "    train_corpus = train_df[feature]\n",
    "    test = test_df[feature]\n",
    "    # tokenize a training corpus\n",
    "    corpus = list(tokenize_corpus(train_corpus))\n",
    "    \n",
    "    # train doc2vec on the training corpus\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=40)\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # tokenize new documents\n",
    "    doc_train = list(tokenize_corpus(train_corpus, tokens_only=True, ngram=ngram))\n",
    "    doc_test = list(tokenize_corpus(test, tokens_only=True, ngram=ngram))\n",
    "    \n",
    "    # generate embeddings for the new documents in training set\n",
    "    x_train = np.zeros((len(doc_train),vec_size))\n",
    "    for i in range(len(doc_train)):\n",
    "        x_train[i,:] = model.infer_vector(doc_train[i])\n",
    "\n",
    "    # generate embeddings for the new documents in test set\n",
    "    x_test = np.zeros((len(doc_test),vec_size))  # type = numpy.ndarray\n",
    "    for i in range(len(doc_test)):\n",
    "        x_test[i,:] = model.infer_vector(doc_test[i])\n",
    "\n",
    "    # print(x_test)\n",
    "    # check the shape of doc_emb\n",
    "    # print(x_train.shape)\n",
    "    np.savetxt(f'./preprocessed_data/doc2Vec/train_{feature}_{ngram}grams_doc2vec.csv', x_train, delimiter=',')\n",
    "    np.savetxt(f'./preprocessed_data/doc2Vec/test_{feature}_{ngram}grams_doc2vec.csv', x_test, delimiter=',')\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5550f",
   "metadata": {},
   "source": [
    "## M3: TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a80cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_tfidf_vectorizer(feature, train_df, test_df, ngram=1):\n",
    "    train_corpus = train_df[feature]\n",
    "    test = test_df[feature]\n",
    "    \n",
    "    vocab = TfidfVectorizer(preprocessor=preprocess, min_df=2, max_df=len(train_df), ngram_range=(ngram,ngram), max_features=300).fit(train_corpus)\n",
    "    x_train = vocab.transform(train_corpus)\n",
    "    x_test = vocab.transform(test)\n",
    "    \n",
    "    scipy.sparse.save_npz(f'./preprocessed_data/TfIdf/train_{feature}_{ngram}grams_tfidf.npz', x_train)\n",
    "    scipy.sparse.save_npz(f'./preprocessed_data/TfIdf/test_{feature}_{ngram}grams_tfidf.npz', x_test)\n",
    "    \n",
    "    return x_train, x_test, vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01becbf",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75515c4a",
   "metadata": {},
   "source": [
    "## M1: CountVectorizer\n",
    "```python\n",
    "# Check\n",
    "preprocess(train_df['Description'][0])\n",
    "\n",
    "# Process data\n",
    "preprocess_with_count_vectorizer('Name', train_df, test_df, ngram=1)\n",
    "preprocess_with_count_vectorizer('Description', train_df, test_df, ngram=2)\n",
    "preprocess_with_count_vectorizer('Name', train_df, test_df, ngram=2)\n",
    "preprocess_with_count_vectorizer('Description', train_df, test_df, ngram=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c0846",
   "metadata": {},
   "source": [
    "## M2: doc2Vec\n",
    "```python\n",
    "# Check\n",
    "list(tokenize_corpus(train_df['Name'], ngram=1))\n",
    "\n",
    "# Process data\n",
    "preprocess_with_doc2vec('Name', train_df, test_df, ngram=1)\n",
    "preprocess_with_doc2vec('Description', train_df, test_df, ngram=2)\n",
    "preprocess_with_doc2vec('Name', train_df, test_df, ngram=2)\n",
    "preprocess_with_doc2vec('Description', train_df, test_df, ngram=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcff91",
   "metadata": {},
   "source": [
    "## M3: TfIdfVectorizer\n",
    "```python\n",
    "# TfidfVectorizer using the same preprocess() function as CountVectorizer\n",
    "preprocess_with_tfidf_vectorizer('Name', train_df, test_df, ngram=1)\n",
    "preprocess_with_tfidf_vectorizer('Description', train_df, test_df, ngram=2)\n",
    "preprocess_with_tfidf_vectorizer('Name', train_df, test_df, ngram=2)\n",
    "preprocess_with_tfidf_vectorizer('Description', train_df, test_df, ngram=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2468ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## M3: TfIdfVectorizer\n",
    "```python\n",
    "# TfidfVectorizer using the same preprocess() function as CountVectorizer\n",
    "preprocess_with_tfidf_vectorizer('Name', train_df, test_df, ngram=1)\n",
    "preprocess_with_tfidf_vectorizer('Description', train_df, test_df, ngram=2)\n",
    "preprocess_with_tfidf_vectorizer('Name', train_df, test_df, ngram=2)\n",
    "preprocess_with_tfidf_vectorizer('Description', train_df, test_df, ngram=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218549c",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65224a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise page number\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_full_train['pagesNumber'] = scaler.fit_transform(x_full_train['pagesNumber'].values.reshape(-1, 1))\n",
    "\n",
    "# Check if imputation is needed for missing values (no)\n",
    "print(f\"The pagesNumber column has {x_full_train['pagesNumber'].isna().sum()} missing values.\")\n",
    "print(f\"The pagesNumber column has {(x_full_train['pagesNumber'] == np.inf).sum()} infinite values.\")\n",
    "print(f\"The pagesNumber column has {(x_full_train['pagesNumber'] == 0).sum()} zero values.\")  # 196 zero values\n",
    "\n",
    "# Take the log\n",
    "x_full_train['pagesNumber'] = np.log(1 + x_full_train['pagesNumber'])  # prevent zero devision\n",
    "print(x_full_train['pagesNumber'][:20])\n",
    "\n",
    "print(np.mean(x_full_train['pagesNumber']))\n",
    "print(np.std(x_full_train['pagesNumber']))\n",
    "print(np.min(x_full_train['pagesNumber']))\n",
    "print(np.max(x_full_train['pagesNumber']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94764f8d",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b63f09",
   "metadata": {},
   "source": [
    "train_df: original training set, with 'Authors', 'Publisher', 'Language' encoded to numerical attributes\\\n",
    "test_df: original test set, with 'Authors', 'Publisher', 'Language' encoded to numerical attributes\\\n",
    "x_full_train: attributes in train_df\\\n",
    "y_train: class labels (rankings) in train_df\\\n",
    "x_full_test: attributes in test_df\\\n",
    "text_matrix: sparse matrix consisting of the 'Description' and 'Name' feature vectors\\\n",
    "numerical_matrix: matrix containing the (encoded) 'Authors', 'Publisher', 'Language' features\\\n",
    "X_train: fully transformed matrix combining text_matrix and numerical_matrix, representing the training set\\\n",
    "X_test: fully transformed matrix combining text_matrix and numerical_matrix, representing the training set\n",
    "\n",
    "\n",
    "Dimensionality reduction: Truncated singular value decomposition (SVD)\\\n",
    "Unlike PCA, this estimator does not center the data before computing the SVD. This means it can work with sparse matrices efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6120ca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a8486b9f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_matrix(description_file, name_file, numerical_file, file_type='npz', dimensionality_reduction=True):\n",
    "    \"\"\"\n",
    "    Reads in and combines the text and numerical features into a single matrix;\n",
    "    Returns the combined matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # text features: 'Description' and 'Name'\n",
    "    if file_type == 'npz':\n",
    "        name_matrix = sparse.load_npz(name_file)\n",
    "        description_matrix = sparse.load_npz(description_file)\n",
    "        text_matrix = sparse.hstack((name_matrix, description_matrix))  # bind sparse matrices\n",
    "    else:\n",
    "        name_matrix = np.loadtxt(open(name_file, \"rb\"), delimiter=\",\")\n",
    "        description_matrix = np.loadtxt(open(description_file, \"rb\"), delimiter=\",\")\n",
    "        text_matrix = np.hstack((name_matrix, description_matrix))  # bind np arrays\n",
    "    \n",
    "    if dimensionality_reduction:\n",
    "        # Dimensionality reduction on text features\n",
    "        lda = LatentDirichletAllocation(n_components=20, random_state=30027)\n",
    "        name_matrix = lda.fit_transform(name_matrix)\n",
    "        text_matrix = lda.fit_transform(text_matrix)\n",
    "        #svd = TruncatedSVD(n_components=10, random_state=30027)\n",
    "        #name_matrix = svd.fit_transform(name_matrix)\n",
    "        #text_matrix = svd.fit_transform(text_matrix)\n",
    "    \n",
    "    numerical_matrix = scipy.sparse.load_npz(numerical_file)\n",
    "    X = sparse.hstack((numerical_matrix, text_matrix))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "90d91e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "DESCRIPTION_TRAIN_SET = './preprocessed_data/TfIdf/train_Description_2grams_tfidf.npz'\n",
    "NAME_TRAIN_SET = './preprocessed_data/TfIdf/train_Name_2grams_tfidf.npz'\n",
    "NUMERICAL_TRAIN_SET = './preprocessed_data/train_numerical.npz'\n",
    "\n",
    "DESCRIPTION_TEST_SET = './preprocessed_data/TfIdf/test_Description_2grams_tfidf.npz'\n",
    "NAME_TEST_SET = './preprocessed_data/TfIdf/test_Name_2grams_tfidf.npz'\n",
    "NUMERICAL_TEST_SET = './preprocessed_data/test_numerical.npz'\n",
    "FILE_TYPE = 'npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f33fd621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23063, 189536)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full_train = transform_to_matrix(DESCRIPTION_TRAIN_SET, NAME_TRAIN_SET, NUMERICAL_TRAIN_SET, file_type=FILE_TYPE, dimensionality_reduction=False)\n",
    "#X_test = transform_to_matrix(DESCRIPTION_TEST_SET, NAME_TEST_SET, NUMERICAL_TEST_SET, file_type=FILE_TYPE)\n",
    "X_full_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88372cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
