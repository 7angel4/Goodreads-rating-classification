{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import chi2, SelectKBest, SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d030454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run TextPreprocessing.ipynb\n",
    "%run HelperFunctions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81812266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(r\"book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "test_df = pd.read_csv(r\"book_rating_test.csv\", index_col = False, delimiter = ',', header=0)\n",
    "entire_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f55cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"./datasets\"\n",
    "CLASS_LABEL = \"rating_label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e25a82",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4fe86",
   "metadata": {},
   "source": [
    "Note: Here we show the individual preprocessing steps for the training set for clarity. At the end we pull everything together to preprocess the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b1fa6",
   "metadata": {},
   "source": [
    "## Preprocess String features: Authors and Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = ['Authors', 'Publisher']\n",
    "MISSING_CAT_VAL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ab1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('Language', axis=1)  # this feature has too many missing values\n",
    "\n",
    "for df in [train_df, entire_df]:\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].fillna(MISSING_CAT_VAL)  # impute missing values\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(x, stop_words_removal=False, lemmatize=False, min_word_len=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d720b85",
   "metadata": {},
   "source": [
    "### Version 1: One-Hot Encoding\n",
    "Some models will use different encodings of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d64489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "OHE = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=3, sparse=False)\n",
    "OHE.fit(entire_df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_transform(df):\n",
    "    \"\"\"\n",
    "    Transforms the dataset by one-hot-encoding, on the categorical features only.\n",
    "    Returns the transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformed_mat = OHE.transform(df[CATEGORICAL_FEATURES])\n",
    "    transformed_cat_df = pd.DataFrame(transformed_mat).set_axis(OHE.get_feature_names_out(), axis=1, inplace=False)\n",
    "    transformed_df = pd.concat([df.reset_index(drop=True), transformed_cat_df], axis=1)\n",
    "    transformed_df = transformed_df.drop(CATEGORICAL_FEATURES, axis=1)  # drop the original attributes\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "# Select one-hot-encoded features using chi2\n",
    "x2 = SelectKBest(chi2, k='all')\n",
    "x2.fit(ohe_transform(train_df)[OHE.get_feature_names_out()], train_df[CLASS_LABEL])\n",
    "pvals = pd.DataFrame(x2.pvalues_, index=x2.feature_names_in_, columns=['p-value'])\n",
    "# print(pvals)\n",
    "\n",
    "INSIG_OHE_FEATURES = pvals[pvals['p-value'] >= ALPHA].index.tolist()  # insignificant encoded features\n",
    "sig_ohe_features = pvals[pvals['p-value'] < ALPHA].index.tolist()\n",
    "print(len(sig_ohe_features), \"features are significant.\")\n",
    "sig_ohe_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7626436",
   "metadata": {},
   "source": [
    "### Version 2: Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e77571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ORD_ENCODER = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "ORD_ENCODER.fit(entire_df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ord_enc_transform(df):\n",
    "    \"\"\"\n",
    "    Transforms the dataset by one-hot-encoding, on the categorical features only.\n",
    "    Returns the transformed DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[CATEGORICAL_FEATURES] = ORD_ENCODER.transform(df[CATEGORICAL_FEATURES])\n",
    "    for i in range(len(CATEGORICAL_FEATURES)):  \n",
    "        # some models (e.g. CategoricalNB), cannot handle negative values, so we replace -1 with the next unused int to denote unseen values\n",
    "        df[CATEGORICAL_FEATURES[i]] = df[CATEGORICAL_FEATURES[i]].replace(-1, ORD_ENCODER.categories_[i].size)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e29f9",
   "metadata": {},
   "source": [
    "## Discretize 'Numerical' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58895b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize(df, feature, discretizer=None, strategy='kmeans', n_bins=5):\n",
    "    \"\"\"\n",
    "    Discretizes the feature in the given DataFrame.\n",
    "    - discretizer: the discretizer; \n",
    "      If None, creates a KBinsDiscretizer for ordinal data, with the specified strategy and number of bins.\n",
    "    - strategy: the discretization strategy (one of ['kmeans', 'quantile', 'uniform']).\n",
    "    - n_bins: the number of bins.\n",
    "    Returns the transformed dataset and discretizer used.\n",
    "    \"\"\"\n",
    "    \n",
    "    if discretizer is None:\n",
    "        discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "        discretizer.fit(df[[feature]])\n",
    "        \n",
    "    transformed_df = df.copy()\n",
    "    transformed_df[feature] = discretizer.transform(df[[feature]])\n",
    "    \n",
    "    return transformed_df, discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PublishYear\n",
    "train_df, DISCRETIZER_PY = discretize(train_df, 'PublishYear', strategy='uniform', n_bins=15)\n",
    "\n",
    "# PublishMonth\n",
    "train_df, DISCRETIZER_PM = discretize(train_df, 'PublishMonth', strategy='quantile', n_bins=6)\n",
    "\n",
    "# PublishDay\n",
    "train_df, DISCRETIZER_PD = discretize(train_df, 'PublishDay', strategy='quantile', n_bins=11)\n",
    "\n",
    "# pagesNumber\n",
    "train_df, DISCRETIZER_PN = discretize(train_df, 'pagesNumber', strategy='kmeans', n_bins=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95929a9",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_name, VECTORIZER_NAME = preprocess_text_feature(train_df, 'Name', ngram=2, delimiter='_')  # can try ngram=1\n",
    "train_df_desc, VECTORIZER_DESC = preprocess_text_feature(train_df, 'Description', ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind them together\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_df_name, train_df_desc], axis=1)\n",
    "train_df = train_df.drop(TEXT_FEATURES, axis=1)  # drop the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7247bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, move rating_label to the last column\n",
    "labels = train_df[CLASS_LABEL].astype('category')\n",
    "train_df = pd.concat([train_df.drop([CLASS_LABEL], axis=1), labels], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a06799",
   "metadata": {},
   "source": [
    "## Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_df(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # preprocess string features\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].fillna(MISSING_CAT_VAL)  # imputation\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(str(x), stop_words_removal=False, lemmatize=False, min_word_len=0))\n",
    "    \n",
    "    # preprocess numerical features\n",
    "    df = discretize(df, 'PublishDay', discretizer=DISCRETIZER_PD)[0]\n",
    "    df = discretize(df, 'PublishMonth', discretizer=DISCRETIZER_PM)[0]\n",
    "    df = discretize(df, 'PublishYear', discretizer=DISCRETIZER_PY)[0]\n",
    "    df = discretize(df, 'pagesNumber', discretizer=DISCRETIZER_PN)[0]\n",
    "    \n",
    "    # preprocess text features\n",
    "    df_name = preprocess_text_feature(df, 'Name', vectorizer=VECTORIZER_NAME, delimiter='_')[0]\n",
    "    df_desc = preprocess_text_feature(df, 'Description', vectorizer=VECTORIZER_DESC)[0]\n",
    "    \n",
    "    # discard the obsolete original features and unwanted features\n",
    "    df = pd.concat([df.reset_index(drop=True), df_name, df_desc], axis=1)\n",
    "    df = df.drop(['Name', 'Description', 'Language'], axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we save the preprocessed datasets\n",
    "train_df.to_csv(DATASET_DIR + \"/train_df.csv\", index=False)\n",
    "\n",
    "# Encoded versions\n",
    "train_df_ohe = ohe_transform(train_df).drop(INSIG_OHE_FEATURES, axis=1)  # drop the 'insignificant' OHE features\n",
    "train_df_oe = ord_enc_transform(train_df)\n",
    "train_df_ohe.to_csv(DATASET_DIR + \"/train_df_ohe.csv\", index=False)\n",
    "train_df_oe.to_csv(DATASET_DIR + \"/train_df_oe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fdc3b1",
   "metadata": {},
   "source": [
    "# 2. Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a9d61",
   "metadata": {},
   "source": [
    "For the first 3 models, we use the one-hot-encoded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a55ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_ohe = pd.read_csv(DATASET_DIR + \"/train_df_ohe.csv\", keep_default_na=False)\n",
    "X_train = train_df_ohe.iloc[:,:-1]\n",
    "y_train = train_df_ohe.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f1c4a",
   "metadata": {},
   "source": [
    "## 1) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208876cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression(solver='sag', max_iter=500)  # uses Stochastic Average Gradient descent solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(logr, prefit=True).fit(X, y)\n",
    "LR_FEATURES = [X_train.columns[i] for i in selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ae062",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.fit(X_train[LR_FEATURES], y_train)\n",
    "y_pred_logr, logr_report = cross_val_report(logr, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe892a9",
   "metadata": {},
   "source": [
    "## 2) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_FEATURES = chi2_select_features(X_train, y_train).remove('PublishDay')\n",
    "rf = RandomForestClassifier(random_state=30027, \n",
    "                            max_samples=0.3, max_features='log2', \n",
    "                            criterion='entropy', n_estimators=100, \n",
    "                            oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train[RF_FEATURES], y_train)\n",
    "y_pred_rf, cross_val_report(rf, X_train[RF_FEATURES], y_train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbf9fe",
   "metadata": {},
   "source": [
    "## 3) StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa832b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42773311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian NB\n",
    "gnb = GaussianNB()\n",
    "# Linear SVM\n",
    "linearSVM = svm.LinearSVC(random_state=30027, C=1)\n",
    "# Logistic Regression\n",
    "logr = LogisticRegression(solver='sag', max_iter=100)\n",
    "# Decision tree\n",
    "dt = DecisionTreeClassifier(max_depth = 400, random_state = 30027)\n",
    "# 3NN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Stack\n",
    "sclf = StackingCVClassifier(classifiers=[gnb, linearSVM, logr, dt, knn], \n",
    "                            meta_classifier=LogisticRegression(),\n",
    "                            cv=2,\n",
    "                            random_state=30027)\n",
    "\n",
    "sclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fc760",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sclf, sclf_report = cross_val_report(sclf, X_train, y_train)\n",
    "sclf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d7b7e",
   "metadata": {},
   "source": [
    "## 4) Stacking Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70cc213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_oe = pd.read_csv(DATASET_DIR + \"/train_df_oe.csv\", keep_default_na=False)\n",
    "y_train_oe = train_df_oe.iloc[:,-1]\n",
    "X_train_oe = train_df_oe.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAUS_FEATURES = ['pagesNumber', 'PublishYear']\n",
    "CAT_FEATURES = ['Authors']\n",
    "MN_FEATURES = chi2_select_features(X_train_oe[X.columns[6:]], y_train_oe)  # only filtering the text features\n",
    "N_AUTHORS_VALS = len(ORD_ENCODER.categories_[0])  # number of unique Authors categories\n",
    "\n",
    "X_train_oe = X_train_oe[GAUS_FEATURES + CAT_FEATURES + MN_FEATURES]\n",
    "X_train_oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = make_pipeline(ColumnSelector(GAUS_FEATURES),\n",
    "                      GaussianNB())\n",
    "cnb = make_pipeline(ColumnSelector(CAT_FEATURES),\n",
    "                      CategoricalNB(alpha=0.11, min_categories=N_AUTHORS_VALS))\n",
    "mnb = make_pipeline(ColumnSelector(MN_FEATURES),\n",
    "                      MultinomialNB(alpha=0.26))\n",
    "\n",
    "cvsnb = StackingCVClassifier(classifiers=[gnb, cnb, mnb], \n",
    "                            meta_classifier=CategoricalNB(),\n",
    "                            random_state=30027)\n",
    "\n",
    "cvsnb.fit(X_train_oe, y_train_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cvsnb, cvsnb_report = cross_val_report(cvsnb, X_train_oe, y_train_oe)\n",
    "cvsnb_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f522f",
   "metadata": {},
   "source": [
    "# 3. Final predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f48f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preprocess_df(test_df)\n",
    "test_df_ohe = ohe_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea5330a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ohe_transform(test_df).drop(INSIG_OHE_FEATURES, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2dd47b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_sets \u001b[38;5;241m=\u001b[39m {\u001b[43msnb\u001b[49m: X_test_oe, RBF_svm: X_test_ohe, logr: X_test_ohe, hgb: X_test_oe}\n\u001b[1;32m      2\u001b[0m y_test_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_sets:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snb' is not defined"
     ]
    }
   ],
   "source": [
    "sclf.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
