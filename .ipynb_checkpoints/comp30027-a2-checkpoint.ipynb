{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Global.ipynb\n",
    "%run Text_Preprocessing.ipynb\n",
    "%run Helper_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation:\n",
    "1. Converted class label to categorical\n",
    "2. Supplied (original) vectorisers + TF-IDF vectorizer\n",
    "3. Added preprocessing (punctuation removal, casefolding, stop-words removal (including words to short), lemmatization) steps to the text features\n",
    "4. Encoded the simple string features ('Authors', 'Publisher', 'Language')\n",
    "5. Dimensionality reduction with TruncatedSVD\n",
    "6. Bigrams instead of words\n",
    "7. Feature selection with SelectKBest\n",
    "8. Standardization of 'pageNumbers' \n",
    "9. Instead of 8: Take the log of the standardized 'pageNumbers' using MinMaxScaler\n",
    "\n",
    "Yet to try:\n",
    "* Discretisation of PageNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'book_rating_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m original_train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbook_rating_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m original_test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbook_rating_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, delimiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(original_train_df))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'book_rating_train.csv'"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "original_train_df = pd.read_csv(r\"book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "original_test_df = pd.read_csv(r\"book_rating_test.csv\", index_col = False, delimiter = ',', header=0)\n",
    "print(\"Training set size:\", len(original_train_df))\n",
    "print(\"Test set size:\", len(original_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = original_train_df.copy()\n",
    "test_df = original_test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_FEATURES = ['Authors', 'Publisher', 'Language']  # categorical\n",
    "NUMERICAL_FEATURES = ['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']  # not necessarily continuous, but are 'numbers'\n",
    "TEXT_FEATURES = ['Name', 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distributions\n",
    "class_labels, counts = np.unique(original_train_df[CLASS_LABEL], return_counts=True)\n",
    "for label, count in zip(class_labels, counts):\n",
    "    print(f\"Rating {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess String features: Language, Authors, and Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in STRING_FEATURES:\n",
    "    print(f\"There are {original_train_df[feature].nunique()} unique '{feature}' values.\")\n",
    "    print(f\"There are {original_train_df[feature].isna().sum()} missing '{feature}' values.\\n\")\n",
    "\n",
    "# Too many missing values in 'Language'. Let's drop it.\n",
    "train_df = train_df.drop('Language', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Authors and Publisher, we just do a general text preprocessing here\n",
    "# The models will later choose their own version of preprocessed dataset\n",
    "CATEGORICAL_FEATURES = ['Authors', 'Publisher']\n",
    "MISSING_CAT_VAL = ''\n",
    "entire_df = pd.concat([train_df, test_df])\n",
    "\n",
    "for df in [train_df, entire_df]:\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].fillna(MISSING_CAT_VAL)  # imputation\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(x, stop_words_removal=False, lemmatize=False, min_word_len=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "OHE = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=3, sparse=False)\n",
    "OHE.fit(entire_df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_transform(df, has_labels=False):\n",
    "    \"\"\"\n",
    "    Transforms the dataset by one-hot-encoding (on the categorical features).\n",
    "    `has_label` indicates whether the DataFrame contains the class labels.\n",
    "        If True, these are moved to the last column.\n",
    "    Returns the transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformed_mat = OHE.transform(df[CATEGORICAL_FEATURES])\n",
    "    transformed_cat_df = pd.DataFrame(transformed_mat).set_axis(OHE.get_feature_names_out(), axis=1, inplace=False)\n",
    "    transformed_df = pd.concat([df.reset_index(drop=True), transformed_cat_df], axis=1)\n",
    "    transformed_df = transformed_df.drop(CATEGORICAL_FEATURES, axis=1)  # drop the original attributes\n",
    "    \n",
    "    if has_labels:\n",
    "        # move rating_label to the last column\n",
    "        labels = transformed_df[CLASS_LABEL]\n",
    "        transformed_df = pd.concat([transformed_df.drop([CLASS_LABEL], axis=1), labels], axis=1)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one-hot-encoded features using chi2\n",
    "chi2_selector = SelectKBest(chi2, k='all')\n",
    "chi2_selector.fit(ohe_transform(train_df)[OHE.get_feature_names_out()], train_df[CLASS_LABEL])\n",
    "pvals = pd.DataFrame(chi2_selector.pvalues_, index=chi2_selector.feature_names_in_, columns=['p-value'])\n",
    "pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember these features for later\n",
    "insig_cat_features_ohe = pvals[pvals['p-value'] >= ALPHA].index.tolist()\n",
    "sig_cat_features_ohe = pvals[pvals['p-value'] < ALPHA].index.tolist()\n",
    "print(len(sig_cat_features_ohe), \"features are significant.\")\n",
    "sig_cat_features_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Ordinal Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ORD_ENCODER = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "ORD_ENCODER.fit(entire_df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ord_enc_transform(df):\n",
    "    df = df.copy()\n",
    "    df[CATEGORICAL_FEATURES] = ORD_ENCODER.transform(df[CATEGORICAL_FEATURES])\n",
    "    for i in range(len(CATEGORICAL_FEATURES)):  \n",
    "        # some models (e.g. CategoricalNB), cannot handle negative values, so we replace -1 with the next unused int to denote unseen values\n",
    "        df[CATEGORICAL_FEATURES[i]] = df[CATEGORICAL_FEATURES[i]].replace(-1, ORD_ENCODER.categories_[i].size)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chi2\n",
    "chi2_selector = SelectKBest(chi2, k='all')\n",
    "chi2_selector.fit(ord_enc_transform(train_df)[['Authors', 'Publisher']], train_df[CLASS_LABEL])\n",
    "pd.DataFrame(chi2_selector.pvalues_, index=chi2_selector.feature_names_in_, columns=['p-value'])\n",
    "# Looks like we can keep both features using ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(ORD_ENCODER.categories_[i]) for i in range(len(CATEGORICAL_FEATURES))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Numerical features: PublishDates and pagesNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T08:06:46.337254Z",
     "iopub.status.busy": "2023-04-16T08:06:46.336497Z",
     "iopub.status.idle": "2023-04-16T08:06:47.572378Z",
     "shell.execute_reply": "2023-04-16T08:06:47.571097Z",
     "shell.execute_reply.started": "2023-04-16T08:06:46.337212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplots for Publish year, month, and day vs. Rating\n",
    "for feature in ['PublishYear', 'PublishMonth', 'PublishDay']: \n",
    "    scatter_vs_rating(original_train_df, feature)\n",
    "    \n",
    "    # Look at the average rating for each value\n",
    "    values, counts = np.unique(sorted(original_train_df[feature]), return_counts=True)\n",
    "    avg = [sum(original_train_df.loc[original_train_df[feature] == values[i]][CLASS_LABEL]) / counts[i] for i in range(len(values))]\n",
    "    plt.plot(values, avg, color = 'red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for pages number < 2000\n",
    "hist_plot(original_train_df.loc[original_train_df['pagesNumber'] < 2000], 'pagesNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(original_train_df, 'pagesNumber')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T08:06:40.804818Z",
     "iopub.status.busy": "2023-04-16T08:06:40.803332Z",
     "iopub.status.idle": "2023-04-16T08:06:41.319477Z",
     "shell.execute_reply": "2023-04-16T08:06:41.317637Z",
     "shell.execute_reply.started": "2023-04-16T08:06:40.804753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the correlation matrix of the numerical attributes\n",
    "cor_matrix = original_train_df[NUMERICAL_FEATURES + [CLASS_LABEL]].corr()\n",
    "round(cor_matrix, 2)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cor_matrix, cmap='cividis', annot=True, linewidths=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order: ['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']\n",
    "mutual_info_classif(train_df[NUMERICAL_FEATURES], train_df[CLASS_LABEL], discrete_features=[True, True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in NUMERICAL_FEATURES:\n",
    "    print(f\"AMI for {feature} =\", adjusted_mutual_info_score(train_df[feature], train_df[CLASS_LABEL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI: top 2 = PublishYear, pagesNumber\n",
    "# Correlation matrix: top 2 = PublishYear, pagesNumber\n",
    "# Discard PublishMonth, PublishDay\n",
    "#train_df = train_df.drop(['PublishMonth', 'PublishDay'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try discretizing it\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize(df, feature, discretizer=None, strategy='kmeans', n_bins=5):\n",
    "    if discretizer is None:\n",
    "        discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "        discretizer.fit(df[[feature]])\n",
    "        \n",
    "    transformed_df = df.copy()\n",
    "    transformed_df[feature] = discretizer.transform(df[[feature]])\n",
    "    \n",
    "    return transformed_df, discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRETIZATION_STRATEGIES = ['kmeans', 'quantile', 'uniform']\n",
    "\n",
    "def tune_discretization(X, y, feature, lower, upper):\n",
    "    \"\"\"\n",
    "    Returns the combination of n (number of bins) and discretisation strategy \n",
    "    that gives the highest normalised mutual information between `X` and `y`.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_ami = 0\n",
    "    best_n = 0\n",
    "    best_strategy = None\n",
    "    \n",
    "    for n in range(lower, upper+1):\n",
    "        for strategy in DISCRETIZATION_STRATEGIES:\n",
    "            transformed_X, _ = discretize(X, feature, strategy=strategy, n_bins=n)\n",
    "            ami = adjusted_mutual_info_score(transformed_X[feature], y)\n",
    "            if ami > max_ami:\n",
    "                max_ami = ami\n",
    "                best_n = n\n",
    "                best_strategy = strategy\n",
    "    \n",
    "    return max_ami, best_n, best_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins_upper = {'PublishYear': train_df['PublishYear'].nunique(), 'PublishMonth': 12, 'PublishDay': 31, 'pagesNumber': 30}\n",
    "\n",
    "for feature in NUMERICAL_FEATURES:\n",
    "    max_ami, best_n, best_strategy = tune_discretization(train_df, train_df[CLASS_LABEL], feature, 2, nbins_upper[feature])\n",
    "    print(\"Feature:\", feature)\n",
    "    print(\"The number of bins that gives the highest AMI with rating_label is\", best_n)\n",
    "    print(\"The highest AMI is:\", max_ami)\n",
    "    print(\"The best strategy is:\", best_strategy)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# All AMI increased slightly after discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PY = discretize(train_df, 'PublishYear', strategy='uniform', n_bins=15)\n",
    "np.unique(train_df['PublishYear'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'PublishYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PM = discretize(train_df, 'PublishMonth', strategy='quantile', n_bins=6)\n",
    "np.unique(train_df['PublishMonth'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'PublishMonth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PD = discretize(train_df, 'PublishDay', strategy='quantile', n_bins=11)\n",
    "np.unique(train_df['PublishDay'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'PublishDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PN = discretize(train_df, 'pagesNumber', strategy='kmeans', n_bins=7)\n",
    "np.unique(train_df['pagesNumber'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'pagesNumber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a different delimiter for the name and description bigrams, to prevent duplicate column namaes\n",
    "train_df_name, VECTORIZER_NAME = preprocess_text_feature(train_df, 'Name', ngram=2, delimiter='_', max_features=300)  # can try ngram=1\n",
    "train_df_desc, VECTORIZER_DESC = preprocess_text_feature(train_df, 'Description', ngram=2, max_features=300)\n",
    "# print(sorted(vectorizer_name.vocabulary_))\n",
    "# print(sorted(vectorizer_desc.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bind them together\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_df_name, train_df_desc], axis=1)\n",
    "train_df = train_df.drop(TEXT_FEATURES, axis=1)  # drop the original columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert rating_label to categorical class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finally, convert rating_label to categorical class label\n",
    "train_df[CLASS_LABEL] = train_df[CLASS_LABEL].astype('category')\n",
    "# move rating_label to the last column\n",
    "labels = train_df[CLASS_LABEL]\n",
    "train_df = pd.concat([train_df.drop([CLASS_LABEL], axis=1), labels], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = train_df.dtypes.apply(lambda x: x.name).to_dict()  # needed for reading the CSV later\n",
    "DTYPE['Authors'] = 'string'\n",
    "DTYPE['Publisher'] = 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the general transformed df\n",
    "train_df.to_csv(DATASET_DIR + \"/train_df_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_df(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # preprocess string features\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].fillna(MISSING_CAT_VAL)  # imputation\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(str(x), stop_words_removal=False, lemmatize=False, min_word_len=0))\n",
    "    \n",
    "    # preprocess pagesNumber and PublishYear\n",
    "    df = discretize(df, 'PublishDay', discretizer=DISCRETIZER_PD)[0]\n",
    "    df = discretize(df, 'PublishMonth', discretizer=DISCRETIZER_PM)[0]\n",
    "    df = discretize(df, 'PublishYear', discretizer=DISCRETIZER_PY)[0]\n",
    "    df = discretize(df, 'pagesNumber', discretizer=DISCRETIZER_PN)[0]\n",
    "    \n",
    "    # preprocess text features\n",
    "    df_name = preprocess_text_feature(df, 'Name', vectorizer=VECTORIZER_NAME, delimiter='_', max_features=300)[0]\n",
    "    df_desc = preprocess_text_feature(df, 'Description', vectorizer=VECTORIZER_DESC, max_features=300)[0]\n",
    "    \n",
    "    # discard the obsolete original features and unwanted features\n",
    "    df = pd.concat([df.reset_index(drop=True), df_name, df_desc], axis=1)\n",
    "    df = df.drop(['Name', 'Description', 'Language'], axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preprocess_test_df(original_test_df)\n",
    "test_df.to_csv(DATASET_DIR + \"/test_df.csv\", index=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we keep four versions of the transformed datasets\n",
    "# a) Original\n",
    "# b) Using one-hot encoding, with the full set of 'Authors' and 'Publisher' features\n",
    "# c) Using one-hot encoding, with the chi2-selected set of 'Authors' and 'Publisher' features\n",
    "# d) Using ordinal encoding\n",
    "# Each model can choose their own version of dataset\n",
    "train_df_ohe_full = ohe_transform(train_df, has_labels=True)\n",
    "train_df_ohe_selected = train_df_ohe_full.drop(insig_cat_features_ohe, axis=1)  # drop the 'insignificant' OHE features\n",
    "train_df_oe = ord_enc_transform(train_df)\n",
    "\n",
    "train_df_ohe_full.to_csv(DATASET_DIR + \"/train_df_ohe_full.csv\", index=False)\n",
    "train_df_ohe_selected.to_csv(DATASET_DIR + \"/train_df_ohe_selected.csv\", index=False)\n",
    "train_df_oe.to_csv(DATASET_DIR + \"/train_df_oe_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ohe_full = ohe_transform(test_df)\n",
    "test_df_ohe_selected = test_df_ohe_full.drop(insig_cat_features_ohe, axis=1)  # drop the 'insignificant' OHE features\n",
    "test_df_oe = ord_enc_transform(test_df)\n",
    "\n",
    "test_df_ohe_full.to_csv(DATASET_DIR + \"/test_df_ohe_full.csv\", index=False)\n",
    "test_df_ohe_selected.to_csv(DATASET_DIR + \"/test_df_ohe_selected.csv\", index=False)\n",
    "test_df_oe.to_csv(DATASET_DIR + \"/test_df_oe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = ohe_transform(test_df)\n",
    "X_test_oe = ord_enc_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sets = {snb: X_test_oe, RBF_svm: X_test_ohe, logr: X_test_ohe, hgb: X_test_oe}\n",
    "y_test_preds = []\n",
    "for model in model_sets:\n",
    "    y_test_pred = model.predict(model_sets[model])\n",
    "    y_test_preds.append(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
