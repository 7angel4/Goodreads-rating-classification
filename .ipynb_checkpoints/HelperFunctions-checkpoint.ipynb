{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['3.0', '4.0', '5.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01739c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring function for cross validation\n",
    "def report_scoring(clf, X, y, include_avg=False): \n",
    "    \"\"\"\n",
    "    Returns a report of the classifier `clf`'s performance on the provided dataset.\n",
    "    The report is a dictionary, which includes the precision, recall, f1-score, and accuracy.\n",
    "    If `include_avg` is true, the report additionally includes the macro and weighted average \n",
    "    of the precision, recall, and f1-score.\n",
    "    \"\"\"\n",
    "    y_pred = clf.predict(X)\n",
    "    results = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    report_dict = {}  # set up our own output dictionary\n",
    "    for label in CLASSES: \n",
    "        for metric in ['precision', 'recall', 'f1-score']: \n",
    "            report_dict[label + '_' + metric] = results[label][metric]\n",
    "    report_dict['accuracy'] = results['accuracy']\n",
    "    \n",
    "    if include_avg:\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            report_dict['macro_avg_' + metric] = results['macro avg'][metric]\n",
    "            report_dict['weighted_avg_' + metric] = results['weighted avg'][metric]\n",
    "    \n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_report(clf, X, y, cv=10, print_full_results=True, print_confusion_matrix=True):\n",
    "    \"\"\"\n",
    "    Cross-validates the classifier on the given dataset.\n",
    "    - cv: number of folds in a StratifiedKFold cross-validation.\n",
    "    - print_full_results: whether to print the full cross-validation results.\n",
    "    - print_confusion_matrix: whether to print the confusion matrix.\n",
    "    Returns a 2-tuple consisting of:\n",
    "    1) a list for the cross-validated estimates for each input data point\n",
    "    2) a DataFrame for the averaged cross-validation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    # cross validation on TRAINING set\n",
    "    result = cross_validate(clf, X, y, scoring=report_scoring, cv=cv)\n",
    "    end = time.time()\n",
    "    print(f\"{get_model_name(clf)} prediction took {end - start} seconds.\\n\")\n",
    "    \n",
    "    # Convert evaluation results to a dataframe\n",
    "    metrics = ['test_'+c+'_'+m for c in CLASSES for m in ['precision', 'recall', 'f1-score']]\n",
    "    metrics += ['test_accuracy']\n",
    "    results_df = pd.DataFrame.from_dict(result)[metrics].set_axis([metric[5:] for metric in metrics], axis=1)\n",
    "    \n",
    "    if print_full_results:\n",
    "        print(results_df)\n",
    "        \n",
    "    y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "    \n",
    "    if print_confusion_matrix:\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)\n",
    "        display.plot()\n",
    "        plt.show()\n",
    "    \n",
    "    return y_pred, results_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeec879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_select_features(X, y, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Returns the selected features for the dataset using a chi-squared statistical test.\n",
    "    `alpha` is the significance level of the test.\n",
    "    \"\"\"\n",
    "    \n",
    "    x2 = SelectKBest(chi2, k='all')\n",
    "    x2.fit(X, y)\n",
    "    pvals = pd.DataFrame(x2.pvalues_, index=x2.feature_names_in_, columns=['p-value'])\n",
    "    x2_features = pvals[pvals['p-value'] < alpha].index.tolist()  # statistical test\n",
    "    return x2_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
