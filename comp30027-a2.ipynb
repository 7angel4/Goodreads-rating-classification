{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Global.ipynb\n",
    "%run Text_Preprocessing.ipynb\n",
    "%run Helper_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation:\n",
    "1. Converted class label to categorical\n",
    "2. Supplied (original) vectorisers + TF-IDF vectorizer\n",
    "3. Added preprocessing (punctuation removal, casefolding, stop-words removal (including words to short), lemmatization) steps to the text features\n",
    "4. Encoded the simple string features ('Authors', 'Publisher', 'Language')\n",
    "5. Dimensionality reduction with TruncatedSVD\n",
    "6. Bigrams instead of words\n",
    "7. Feature selection with SelectKBest\n",
    "8. Standardization of 'pageNumbers' \n",
    "9. Instead of 8: Take the log of the standardized 'pageNumbers' using MinMaxScaler\n",
    "\n",
    "Yet to try:\n",
    "* Discretisation of PageNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "original_train_df = pd.read_csv(r\"book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "original_test_df = pd.read_csv(r\"book_rating_test.csv\", index_col = False, delimiter = ',', header=0)\n",
    "print(\"Training set size:\", len(original_train_df))\n",
    "print(\"Test set size:\", len(original_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = original_train_df.copy()\n",
    "test_df = original_test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_FEATURES = ['Authors', 'Publisher', 'Language']  # categorical features\n",
    "NUMERICAL_FEATURES = ['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']\n",
    "TEXT_FEATURES = ['Name', 'Description']\n",
    "ALPHA = 0.05  # for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distributions\n",
    "class_labels, counts = np.unique(original_train_df[CLASS_LABEL], return_counts=True)\n",
    "for label, count in zip(class_labels, counts):\n",
    "    print(f\"Rating {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess String features: Language, Authors, and Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in STRING_FEATURES:\n",
    "    print(f\"There are {original_train_df[feature].nunique()} unique '{feature}' values.\")\n",
    "    print(f\"There are {original_train_df[feature].isna().sum()} missing '{feature}' values.\\n\")\n",
    "\n",
    "# Too many missing values in 'Language'. Let's drop it.\n",
    "train_df = train_df.drop('Language', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Authors and Publisher, we just do a general text preprocessing here\n",
    "# The models will later choose their own version of preprocessed dataset\n",
    "CATEGORICAL_FEATURES = ['Authors', 'Publisher']\n",
    "MISSING_CAT_VAL = ''\n",
    "entire_df = pd.concat([train_df, test_df])\n",
    "\n",
    "for df in [train_df, entire_df]:\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].fillna(MISSING_CAT_VAL)\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(x, stop_words_removal=False, lemmatize=False, min_word_len=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "OHE = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=3, sparse=False)\n",
    "OHE.fit(entire_df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_transform(df, has_labels=False):\n",
    "    transformed_mat = OHE.transform(df[CATEGORICAL_FEATURES])\n",
    "    transformed_cat_df = pd.DataFrame(transformed_mat).set_axis(OHE.get_feature_names_out(), axis=1, inplace=False)\n",
    "    transformed_df = pd.concat([df.reset_index(drop=True), transformed_cat_df], axis=1)\n",
    "    transformed_df = transformed_df.drop(CATEGORICAL_FEATURES, axis=1)  # drop the original attributes\n",
    "    \n",
    "    if has_labels:\n",
    "        # move rating_label to the last column\n",
    "        labels = transformed_df[CLASS_LABEL]\n",
    "        transformed_df = pd.concat([transformed_df.drop([CLASS_LABEL], axis=1), labels], axis=1)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chi2\n",
    "chi2_selector = SelectKBest(chi2, k='all')\n",
    "chi2_selector.fit(ohe_transform(train_df)[OHE.get_feature_names_out()], train_df[CLASS_LABEL])\n",
    "pvals = pd.DataFrame(chi2_selector.pvalues_, index=chi2_selector.feature_names_in_, columns=['p-value'])\n",
    "pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember these features for later\n",
    "insig_cat_features_ohe = pvals[pvals['p-value'] >= ALPHA].index.tolist()\n",
    "sig_cat_features_ohe = pvals[pvals['p-value'] < ALPHA].index.tolist()\n",
    "print(len(sig_cat_features_ohe), \"features are significant.\")\n",
    "sig_cat_features_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Ordinal Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ORD_ENCODER = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "ORD_ENCODER.fit(entire_df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ord_enc_transform(df):\n",
    "    df = df.copy()\n",
    "    df[CATEGORICAL_FEATURES] = ORD_ENCODER.transform(df[CATEGORICAL_FEATURES])\n",
    "    for i in range(len(CATEGORICAL_FEATURES)):  \n",
    "        # some models (e.g. CategoricalNB), cannot handle negative values, so we replace -1 with the next unused int to denote unseen values\n",
    "        df[CATEGORICAL_FEATURES[i]] = df[CATEGORICAL_FEATURES[i]].replace(-1, ORD_ENCODER.categories_[i].size)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chi2\n",
    "chi2_selector = SelectKBest(chi2, k='all')\n",
    "chi2_selector.fit(ord_enc_transform(train_df)[['Authors', 'Publisher']], train_df[CLASS_LABEL])\n",
    "pd.DataFrame(chi2_selector.pvalues_, index=chi2_selector.feature_names_in_, columns=['p-value'])\n",
    "# Looks like we can keep both features using ordinal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Numerical features: PublishDates and pagesNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T08:06:46.337254Z",
     "iopub.status.busy": "2023-04-16T08:06:46.336497Z",
     "iopub.status.idle": "2023-04-16T08:06:47.572378Z",
     "shell.execute_reply": "2023-04-16T08:06:47.571097Z",
     "shell.execute_reply.started": "2023-04-16T08:06:46.337212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplots for Publish year, month, and day vs. Rating\n",
    "for feature in ['PublishYear', 'PublishMonth', 'PublishDay']: \n",
    "    scatter_vs_rating(original_train_df, feature)\n",
    "    \n",
    "    # Look at the average rating for each value\n",
    "    values, counts = np.unique(sorted(original_train_df[feature]), return_counts=True)\n",
    "    avg = [sum(original_train_df.loc[original_train_df[feature] == values[i]][CLASS_LABEL]) / counts[i] for i in range(len(values))]\n",
    "    plt.plot(values, avg, color = 'red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for pages number < 2000\n",
    "hist_plot(original_train_df.loc[original_train_df['pagesNumber'] < 2000], 'pagesNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(original_train_df, 'pagesNumber')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T08:06:40.804818Z",
     "iopub.status.busy": "2023-04-16T08:06:40.803332Z",
     "iopub.status.idle": "2023-04-16T08:06:41.319477Z",
     "shell.execute_reply": "2023-04-16T08:06:41.317637Z",
     "shell.execute_reply.started": "2023-04-16T08:06:40.804753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the correlation matrix of the numerical attributes\n",
    "cor_matrix = original_train_df[NUMERICAL_FEATURES + [CLASS_LABEL]].corr()\n",
    "round(cor_matrix, 2)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cor_matrix, cmap='cividis', annot=True, linewidths=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order: ['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']\n",
    "mutual_info_classif(train_df[NUMERICAL_FEATURES], train_df[CLASS_LABEL], discrete_features=[True, True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in NUMERICAL_FEATURES:\n",
    "    print(f\"AMI for {feature} =\", adjusted_mutual_info_score(train_df[feature], train_df[CLASS_LABEL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI: top 2 = PublishYear, pagesNumber\n",
    "# Correlation matrix: top 2 = PublishYear, pagesNumber\n",
    "# Discard PublishMonth, PublishDay\n",
    "#train_df = train_df.drop(['PublishMonth', 'PublishDay'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try discretizing it\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize(df, feature, discretizer=None, strategy='kmeans', n_bins=5):\n",
    "    if discretizer is None:\n",
    "        discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "        discretizer.fit(df[[feature]])\n",
    "        \n",
    "    transformed_df = df.copy()\n",
    "    transformed_df[feature] = discretizer.transform(df[[feature]])\n",
    "    \n",
    "    return transformed_df, discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRETIZATION_STRATEGIES = ['kmeans', 'quantile', 'uniform']\n",
    "\n",
    "def tune_discretization(X, y, feature, lower, upper):\n",
    "    \"\"\"\n",
    "    Returns the combination of n (number of bins) and discretisation strategy \n",
    "    that gives the highest normalised mutual information between `X` and `y`.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_ami = 0\n",
    "    best_n = 0\n",
    "    best_strategy = None\n",
    "    \n",
    "    for n in range(lower, upper+1):\n",
    "        for strategy in DISCRETIZATION_STRATEGIES:\n",
    "            transformed_X, _ = discretize(X, feature, strategy=strategy, n_bins=n)\n",
    "            ami = adjusted_mutual_info_score(transformed_X[feature], y)\n",
    "            if ami > max_ami:\n",
    "                max_ami = ami\n",
    "                best_n = n\n",
    "                best_strategy = strategy\n",
    "    \n",
    "    return max_ami, best_n, best_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins_upper = {'PublishYear': train_df['PublishYear'].nunique(), 'PublishMonth': 12, 'PublishDay': 31, 'pagesNumber': 30}\n",
    "\n",
    "for feature in NUMERICAL_FEATURES:\n",
    "    max_ami, best_n, best_strategy = tune_discretization(train_df, train_df[CLASS_LABEL], feature, 2, nbins_upper[feature])\n",
    "    print(\"Feature:\", feature)\n",
    "    print(\"The number of bins that gives the highest AMI with rating_label is\", best_n)\n",
    "    print(\"The highest AMI is:\", max_ami)\n",
    "    print(\"The best strategy is:\", best_strategy)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# All AMI increased slightly after discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PY = discretize(train_df, 'PublishYear', strategy='uniform', n_bins=15)\n",
    "np.unique(train_df['PublishYear'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'PublishYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PM = discretize(train_df, 'PublishMonth', strategy='quantile', n_bins=6)\n",
    "np.unique(train_df['PublishMonth'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'PublishMonth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PD = discretize(train_df, 'PublishDay', strategy='quantile', n_bins=11)\n",
    "np.unique(train_df['PublishDay'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'PublishDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, DISCRETIZER_PN = discretize(train_df, 'pagesNumber', strategy='kmeans', n_bins=7)\n",
    "np.unique(train_df['pagesNumber'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatter_vs_rating(train_df, 'pagesNumber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a different delimiter for the name and description bigrams, to prevent duplicate column namaes\n",
    "train_df_name, VECTORIZER_NAME = preprocess_text_feature(train_df, 'Name', ngram=2, delimiter='_')  # can try ngram=1\n",
    "train_df_desc, VECTORIZER_DESC = preprocess_text_feature(train_df, 'Description', ngram=2)\n",
    "# print(sorted(vectorizer_name.vocabulary_))\n",
    "# print(sorted(vectorizer_desc.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bind them together\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_df_name, train_df_desc], axis=1)\n",
    "train_df = train_df.drop(TEXT_FEATURES, axis=1)  # drop the original columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert rating_label to categorical class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finally, convert rating_label to categorical class label\n",
    "train_df[CLASS_LABEL] = train_df[CLASS_LABEL].astype('category')\n",
    "# move rating_label to the last column\n",
    "labels = train_df[CLASS_LABEL]\n",
    "train_df = pd.concat([train_df.drop([CLASS_LABEL], axis=1), labels], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = train_df.dtypes.apply(lambda x: x.name).to_dict()  # needed for reading the CSV later\n",
    "DTYPE['Authors'] = 'string'\n",
    "DTYPE['Publisher'] = 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the general transformed df\n",
    "train_df.to_csv(DATASET_DIR + \"/train_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_df(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # preprocess string features\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        df[feature] = df[feature].apply(lambda x: preprocess(str(x), stop_words_removal=False, lemmatize=False, min_word_len=0))\n",
    "    \n",
    "    # preprocess pagesNumber and PublishYear\n",
    "    df = discretize(df, 'PublishDay', discretizer=DISCRETIZER_PD)[0]\n",
    "    df = discretize(df, 'PublishMonth', discretizer=DISCRETIZER_PM)[0]\n",
    "    df = discretize(df, 'PublishYear', discretizer=DISCRETIZER_PY)[0]\n",
    "    df = discretize(df, 'pagesNumber', discretizer=DISCRETIZER_PN)[0]\n",
    "    \n",
    "    # preprocess text features\n",
    "    df_name = preprocess_text_feature(df, 'Name', vectorizer=VECTORIZER_NAME, delimiter='_')[0]\n",
    "    df_desc = preprocess_text_feature(df, 'Description', vectorizer=VECTORIZER_DESC)[0]\n",
    "    \n",
    "    # discard the obsolete original features and unwanted features\n",
    "    df = pd.concat([df.reset_index(drop=True), df_name, df_desc], axis=1)\n",
    "    df = df.drop(['Name', 'Description', 'Language'], axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preprocess_test_df(original_test_df)\n",
    "test_df.to_csv(DATASET_DIR + \"/test_df.csv\", index=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we keep four versions of the transformed datasets\n",
    "# a) Original\n",
    "# b) Using one-hot encoding, with the full set of 'Authors' and 'Publisher' features\n",
    "# c) Using one-hot encoding, with the chi2-selected set of 'Authors' and 'Publisher' features\n",
    "# d) Using ordinal encoding\n",
    "# Each model can choose their own version of dataset\n",
    "train_df_ohe_full = ohe_transform(train_df, has_labels=True)\n",
    "train_df_ohe_selected = train_df_ohe_full.drop(insig_cat_features_ohe, axis=1)  # drop the 'insignificant' OHE features\n",
    "train_df_oe = ord_enc_transform(train_df)\n",
    "\n",
    "train_df_ohe_full.to_csv(DATASET_DIR + \"/train_df_ohe_full.csv\", index=False)\n",
    "train_df_ohe_selected.to_csv(DATASET_DIR + \"/train_df_ohe_selected.csv\", index=False)\n",
    "train_df_oe.to_csv(DATASET_DIR + \"/train_df_oe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ohe_full = ohe_transform(test_df)\n",
    "test_df_ohe_selected = test_df_ohe_full.drop(insig_cat_features_ohe, axis=1)  # drop the 'insignificant' OHE features\n",
    "test_df_oe = ord_enc_transform(test_df)\n",
    "\n",
    "test_df_ohe_full.to_csv(DATASET_DIR + \"/test_df_ohe_full.csv\", index=False)\n",
    "test_df_ohe_selected.to_csv(DATASET_DIR + \"/test_df_ohe_selected.csv\", index=False)\n",
    "test_df_oe.to_csv(DATASET_DIR + \"/test_df_oe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = ohe_transform(test_df)\n",
    "X_test_oe = ord_enc_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sets = {snb: X_test_oe, RBF_svm: X_test_ohe, logr: X_test_ohe, hgb: X_test_oe}\n",
    "y_test_preds = []\n",
    "for model in model_sets:\n",
    "    y_test_pred = model.predict(model_sets[model])\n",
    "    y_test_preds.append(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
